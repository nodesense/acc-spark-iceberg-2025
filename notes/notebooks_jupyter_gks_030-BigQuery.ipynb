{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac954019-de38-4938-903a-cc56fd90988d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/08 05:55:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/02/08 05:55:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/02/08 05:55:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/02/08 05:55:35 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.sql.catalog.iceberg.warehouse', 'gs://tpch-source/iceberg_warehouse'), ('spark.dynamicAllocation.minExecutors', '1'), ('spark.eventLog.enabled', 'true'), ('spark.sql.shuffle.partitions', '1000'), ('spark.dataproc.sql.joinConditionReorder.enabled', 'true'), ('spark.app.initial.jar.urls', 'gs://tpch-source/jars/iceberg-spark-runtime-3.5_2.12-1.7.1.jar'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://cluster-be89-m.local.:8088/proxy/application_1738975127572_0002'), ('spark.ui.proxyBase', '/proxy/application_1738975127572_0002'), ('spark.dataproc.sql.local.rank.pushdown.enabled', 'true'), ('spark.yarn.unmanagedAM.enabled', 'true'), ('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.7-src.zip'), ('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), ('spark.yarn.historyServer.address', 'cluster-be89-m:18080'), ('spark.sql.optimizer.runtime.bloomFilter.join.pattern.enabled', 'true'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.metrics.namespace', 'app_name##${spark.app.name}.app_id##${spark.app.id}'), ('spark.dataproc.sql.optimizer.join.fusion.enabled', 'true'), ('spark.driver.host', 'cluster-be89-m.us-central1-c.c.iceberg-feb-20205.internal'), ('spark.driver.maxResultSize', '1024m'), ('spark.dataproc.sql.optimizer.leftsemijoin.conversion.enabled', 'true'), ('spark.hadoop.hive.execution.engine', 'mr'), ('spark.executor.id', 'driver'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'), ('spark.dynamicAllocation.maxExecutors', '10000'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalogImplementation', 'hive'), ('spark.app.id', 'application_1738975127572_0002'), ('spark.executor.instances', '1'), ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'), ('spark.jars', 'gs://tpch-source/jars/iceberg-spark-runtime-3.5_2.12-1.7.1.jar'), ('spark.sql.cbo.enabled', 'true'), ('spark.dataproc.metrics.listener.metrics.collector.hostname', 'cluster-be89-m'), ('spark.dataproc.sql.parquet.enableFooterCache', 'true'), ('spark.sql.autoBroadcastJoinThreshold', '21m'), ('spark.yarn.am.memory', '640m'), ('spark.checkpoint.compress', 'true'), ('spark.dataproc.advanced.infer.filter.enabled', 'true'), ('spark.yarn.secondary.jars', 'iceberg-spark-runtime-3.5_2.12-1.7.1.jar'), ('spark.dataproc.listeners', 'com.google.cloud.spark.performance.DataprocMetricsListener'), ('spark.app.startTime', '1738994134541'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.deployMode', 'client'), ('spark.yarn.dist.jars', 'gs://tpch-source/jars/iceberg-spark-runtime-3.5_2.12-1.7.1.jar'), ('spark.plugins.defaultList', 'com.google.cloud.dataproc.DataprocSparkPlugin'), ('spark.app.name', 'IcebergAndBigQueryIntegration'), ('spark.history.fs.logDirectory', 'gs://dataproc-temp-us-central1-737313476891-gxzcgknn/915ef1bc-c3bd-4518-81ee-be471ad43ce2/spark-job-history'), ('spark.sql.cbo.joinReorder.enabled', 'true'), ('spark.shuffle.service.enabled', 'true'), ('spark.scheduler.mode', 'FAIR'), ('spark.sql.adaptive.enabled', 'true'), ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'), ('spark.scheduler.minRegisteredResourcesRatio', '0.0'), ('spark.driver.appUIAddress', 'http://cluster-be89-m.us-central1-c.c.iceberg-feb-20205.internal:36627'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'cluster-be89-m.local.'), ('spark.driver.port', '36899'), ('spark.executor.memory', '4176m'), ('spark.repl.local.jars', 'file:/tmp/spark-bff6ee11-83df-4914-bc77-0d791304dac9/iceberg-spark-runtime-3.5_2.12-1.7.1.jar'), ('spark.sql.catalog.iceberg.type', 'hadoop'), ('spark.master', 'yarn'), ('spark.driver.memory', '2048m'), ('spark.ui.port', '0'), ('spark.rpc.message.maxSize', '512'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.eventLog.dir', 'gs://dataproc-temp-us-central1-737313476891-gxzcgknn/915ef1bc-c3bd-4518-81ee-be471ad43ce2/spark-job-history'), ('spark.submit.pyFiles', ''), ('spark.dynamicAllocation.enabled', 'true'), ('spark.app.submitTime', '1738994134356'), ('spark.yarn.isPython', 'true'), ('spark.executor.cores', '1'), ('spark.dataproc.sql.optimizer.scalar.subquery.fusion.enabled', 'true'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "    #.master(\"local[*]\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergAndBigQueryIntegration\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.jars\", \"gs://tpch-source/jars/iceberg-spark-runtime-3.5_2.12-1.7.1.jar\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4176m\") \\\n",
    "    .config(\"spark.driver.memory\", \"2048m\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"gs://tpch-source/iceberg_warehouse\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "print(spark.sparkContext.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3241f10e-36d3-40cf-837c-76246f4f8ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|movieId|               title|            genres|\n",
      "+-------+--------------------+------------------+\n",
      "| 114335|   La cravate (1957)|(no genres listed)|\n",
      "| 122888|      Ben-hur (2016)|(no genres listed)|\n",
      "| 122896|Pirates of the Ca...|(no genres listed)|\n",
      "| 129250|   Superfast! (2015)|(no genres listed)|\n",
      "| 132084| Let It Be Me (1995)|(no genres listed)|\n",
      "| 134861|Trevor Noah: Afri...|(no genres listed)|\n",
      "| 141131|    Guardians (2016)|(no genres listed)|\n",
      "| 141866|   Green Room (2015)|(no genres listed)|\n",
      "| 142456|The Brand New Tes...|(no genres listed)|\n",
      "| 143410|          Hyena Road|(no genres listed)|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from BigQuery table\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", \"iceberg-feb-20205.movies.moviest\") \\\n",
    "    .load()\n",
    "\n",
    "# Show first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83365bcc-9d20-4a05-a1dc-206ad5c0c368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Replace this with your actual DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Movie A\", 2023),\n",
    "    (2, \"Movie B\", 2024),\n",
    "    (3, \"Movie C\", 2022)\n",
    "], [\"movie_id\", \"title\", \"year\"])\n",
    "\n",
    "# Write to BigQuery\n",
    "df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", \"iceberg-feb-20205.movies.moviest2\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"gs://tpch-source/gks/temp/bg/\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5895ed16-d902-4b79-b9a0-e64f3040350e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`iceberg-feb-20205.movies.moviest2`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "df = spark.format(\"bigquery\").table(\"`iceberg-feb-20205.movies.moviest2`\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97adce1-f058-4540-adb5-7842b5a75e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+\n",
      "|movie_id|  title|year|\n",
      "+--------+-------+----+\n",
      "|       1|Movie A|2023|\n",
      "|       2|Movie B|2024|\n",
      "|       3|Movie C|2022|\n",
      "+--------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from BigQuery table\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", \"iceberg-feb-20205.movies.moviest2\") \\\n",
    "    .load()\n",
    "\n",
    "# Show first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc5e3e3-443b-44bd-b5a6-67393db19e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aac07b-1efb-4860-bcc7-fd4298c1581b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}